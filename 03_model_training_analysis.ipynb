{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c3cce924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, TrainingArguments, Trainer, AutoConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b37ecd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def load_json_to_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert DataFrame to Hugging Face Dataset\n",
    "    return Dataset.from_pandas(df)\n",
    "\n",
    "tags_list = [\"O\", \"B-PER\", \"I-PER\", \"B-EMAIL\"]  #one tage for whole email\n",
    "tag_to_id = {tag: i for i, tag in enumerate(tags_list)}\n",
    "id_to_tag = {i: tag for tag, i in tag_to_id.items()}\n",
    "\n",
    "\n",
    "MODEL_NAME = \"prajjwal1/bert-tiny\"  # 4.4M parameters\n",
    "num_labels = len(tags_list)\n",
    "\n",
    "# Load the model configuration\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=num_labels, id2label=id_to_tag, label2id=tag_to_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "def encode_tags(tags, tag_to_id):\n",
    "    return [int(tag_to_id[tag]) for tag in tags] #Converts NER tags from string labels to integer IDs.\n",
    "\n",
    "# Tokenize Data\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "                            examples[\"tokens\"], \n",
    "                            truncation=True, \n",
    "                            is_split_into_words=True, \n",
    "                            padding='max_length',  # Ensures all sequences are the same length  \n",
    "                            max_length=128  # Adjust based on model (BERT: 512, RoBERTa: 256-512, smaller models: 128)\n",
    "                        )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label[word_idx] != 0 else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Prepare Dataset\n",
    "def prepare_dataset(dataset):\n",
    "    dataset = dataset.map(lambda x: {'ner_tags': encode_tags(x['ner_tags'], tag_to_id)})\n",
    "    tokenized_dataset =  dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    \n",
    "    return tokenized_dataset\n",
    "    \n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Flatten the predictions and labels for metric computation\n",
    "    true_predictions = [\n",
    "        pred for pred, label in zip(predictions.flatten(), labels.flatten()) if label != -100\n",
    "    ]\n",
    "    true_labels = [label for label in labels.flatten() if label != -100]\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, true_predictions, average='weighted'\n",
    "    )\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "\n",
    "    # Initialize dictionaries to store metrics for each entity\n",
    "    entity_metrics = {}\n",
    "\n",
    "    # Get unique labels\n",
    "    unique_labels = set(true_labels)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Create binary labels for the current entity\n",
    "        binary_true_labels = [1 if l == label else 0 for l in true_labels]\n",
    "        binary_true_predictions = [1 if p == label else 0 for p in true_predictions]\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(binary_true_labels, binary_true_predictions).ravel()\n",
    "\n",
    "        # Calculate FPR and FNR\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "        # Ensure label is a native Python int\n",
    "        label = int(label)\n",
    "\n",
    "        # Store metrics\n",
    "        entity_metrics[label] = {\n",
    "            'precision': float(precision_score(binary_true_labels, binary_true_predictions)),\n",
    "            'recall': float(recall_score(binary_true_labels, binary_true_predictions)),\n",
    "            'f1': float(f1_score(binary_true_labels, binary_true_predictions)),\n",
    "            'fpr': float(fpr),\n",
    "            'fnr': float(fnr)\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        'accuracy': float(accuracy),\n",
    "        'f1': float(f1),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'entity_metrics': entity_metrics\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Train the Model\n",
    "def train_ner_model(dataset, test_data):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        eval_dataset=test_data,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5b5a69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dataset:\n",
      "Dataset({\n",
      "    features: ['sequence', 'tokens', 'ner_tags'],\n",
      "    num_rows: 28516\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "file_path = \"email_added_data2.json\"  \n",
    "dataset = load_json_to_dataset(file_path)\n",
    "test_data = load_json_to_dataset(\"Internship_task_data/Internship_task_data/test_data.json\")\n",
    "print(\"Loaded Dataset:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "69235fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28516 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28516 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3650 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = prepare_dataset(dataset)\n",
    "test_data = prepare_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "80c8631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zebi2\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10695' max='10695' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10695/10695 5:53:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Entity Metrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.075507</td>\n",
       "      <td>0.975502</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.975283</td>\n",
       "      <td>0.975502</td>\n",
       "      <td>{0: {'precision': 0.9885505948573622, 'recall': 0.9925502536767067, 'f1': 0.9905463867969877, 'fpr': 0.05670299036999493, 'fnr': 0.007449746323293302}, 1: {'precision': 0.9094315245478036, 'recall': 0.8560136203332117, 'f1': 0.8819144271126981, 'fpr': 0.008206893322094221, 'fnr': 0.1439863796667883}, 2: {'precision': 0.9102796478508545, 'recall': 0.9299034519243486, 'f1': 0.9199869152764149, 'fpr': 0.008050837612398058, 'fnr': 0.07009654807565137}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>0.060834</td>\n",
       "      <td>0.979175</td>\n",
       "      <td>0.979040</td>\n",
       "      <td>0.979019</td>\n",
       "      <td>0.979175</td>\n",
       "      <td>{0: {'precision': 0.9895871976257787, 'recall': 0.993629182454563, 'f1': 0.991604071064168, 'fpr': 0.051571211353269135, 'fnr': 0.006370817545437031}, 1: {'precision': 0.9244848329706898, 'recall': 0.8783898820381856, 'f1': 0.9008480917934647, 'fpr': 0.006907370984358902, 'fnr': 0.12161011796181442}, 2: {'precision': 0.9295056238556108, 'recall': 0.9399550324031213, 'f1': 0.9347011244821464, 'fpr': 0.006261762587420712, 'fnr': 0.06004496759687872}}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.058456</td>\n",
       "      <td>0.979923</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.979808</td>\n",
       "      <td>0.979923</td>\n",
       "      <td>{0: {'precision': 0.9899799086290518, 'recall': 0.9936420268447755, 'f1': 0.99180758727676, 'fpr': 0.049607197161682715, 'fnr': 0.006357973155224456}, 1: {'precision': 0.9299011426370523, 'recall': 0.8808220843974218, 'f1': 0.9046964776417685, 'fpr': 0.006392245012644001, 'fnr': 0.11917791560257814}, 2: {'precision': 0.9293506493506494, 'recall': 0.9464356566591721, 'f1': 0.9378153463075815, 'fpr': 0.006319849438881015, 'fnr': 0.053564343340827936}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "W:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "W:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "trainer = train_ner_model(dataset, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "090d7e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='457' max='457' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [457/457 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W:\\anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.09296087175607681, 'eval_accuracy': 0.9708668396715044, 'eval_f1': 0.9705975578101452, 'eval_precision': 0.9705608735485539, 'eval_recall': 0.9708668396715044, 'eval_entity_metrics': {0: {'precision': 0.986765213610731, 'recall': 0.9911759039239612, 'f1': 0.9889656409796358, 'fpr': 0.0655727318803852, 'fnr': 0.00882409607603879}, 1: {'precision': 0.8899634082592787, 'recall': 0.8281649033199562, 'f1': 0.8579527559055118, 'fpr': 0.009857637913271518, 'fnr': 0.1718350966800438}, 2: {'precision': 0.89136024685009, 'recall': 0.9169422034122471, 'f1': 0.9039702718560532, 'fpr': 0.009816677896791283, 'fnr': 0.08305779658775295}}, 'eval_runtime': 10.924, 'eval_samples_per_second': 334.127, 'eval_steps_per_second': 41.835, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2109da1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model3\\\\tokenizer_config.json',\n",
       " './saved_model3\\\\special_tokens_map.json',\n",
       " './saved_model3\\\\vocab.txt',\n",
       " './saved_model3\\\\added_tokens.json',\n",
       " './saved_model3\\\\tokenizer.json')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_directory = \"./saved_model3\" \n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad3792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c87232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ef3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
